{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, mean, std):\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x, y, theta, lamda):\n",
    "    predict = sigmoid(x @ theta)\n",
    "    error = -y * np.log(predict) - (1 - y) * np.log(1 - predict)\n",
    "    temp = theta\n",
    "    temp[0] = 0\n",
    "    temp = temp * temp\n",
    "    extra = temp.sum() * lamda / 2\n",
    "    return (error.sum() + extra) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_gradient(x, y, theta, alpha, lamda):\n",
    "    predictions = alpha * (x.T @ (sigmoid(x @ theta) - y)) / len(y)\n",
    "    temp = theta\n",
    "    temp[0] = 0\n",
    "    temp = temp * lamda / len(y)\n",
    "    return predictions + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, theta, epochs, alpha, lamda):\n",
    "    for i in range(epochs):\n",
    "        theta -= cost_gradient(x, y, theta, alpha, lamda)\n",
    "#         print(\"epoch \" + str(i+1) + \": \", cost(x, y, theta, lamda))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    return sigmoid(x @ theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv', header = None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :-1].values\n",
    "Y = df.iloc[:, -1].values\n",
    "for i in range(x.shape[1]):\n",
    "    me = x.T[i].mean()\n",
    "    std = np.std(x.T[i])\n",
    "    for j in range(x.shape[0]):\n",
    "        x[j][i] = normalise(x[j][i], me, std)\n",
    "X = np.ones(shape = (x.shape[0], x.shape[1] + 1))\n",
    "X[:, 1:] = x\n",
    "Y = Y.reshape((768, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[:600]\n",
    "y_train = Y[:600]\n",
    "x_test = X[600:]\n",
    "y_test = Y[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "l_rate = 0.001\n",
    "theta = np.zeros(shape = (X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    " theta = train(x_train, y_train, theta, epochs, l_rate, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51997986e-04],\n",
       "       [ 8.04900390e-02],\n",
       "       [ 1.84713450e-01],\n",
       "       [ 1.75227261e-02],\n",
       "       [ 2.20427181e-02],\n",
       "       [ 4.89082937e-02],\n",
       "       [ 1.27008320e-01],\n",
       "       [ 6.69979046e-02],\n",
       "       [ 7.62131552e-02]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42191117],\n",
       "        [0.39123127],\n",
       "        [0.44469644],\n",
       "        [0.62568767],\n",
       "        [0.53621027],\n",
       "        [0.47852032],\n",
       "        [0.6508632 ],\n",
       "        [0.37643491],\n",
       "        [0.56918614],\n",
       "        [0.41056759],\n",
       "        [0.4501105 ],\n",
       "        [0.5948713 ],\n",
       "        [0.66917523],\n",
       "        [0.4972929 ],\n",
       "        [0.62272366],\n",
       "        [0.4109577 ],\n",
       "        [0.46334534],\n",
       "        [0.33269801],\n",
       "        [0.56603554],\n",
       "        [0.41381771],\n",
       "        [0.5017408 ],\n",
       "        [0.46227616],\n",
       "        [0.68762247],\n",
       "        [0.46456171],\n",
       "        [0.41391111],\n",
       "        [0.47750772],\n",
       "        [0.40556176],\n",
       "        [0.45738925],\n",
       "        [0.51700852],\n",
       "        [0.3893661 ],\n",
       "        [0.48800712],\n",
       "        [0.4430441 ],\n",
       "        [0.41148036],\n",
       "        [0.45165436],\n",
       "        [0.43457154],\n",
       "        [0.51950076],\n",
       "        [0.46298325],\n",
       "        [0.44034747],\n",
       "        [0.54494755],\n",
       "        [0.37487326],\n",
       "        [0.44549245],\n",
       "        [0.48157188],\n",
       "        [0.53964037],\n",
       "        [0.41316947],\n",
       "        [0.46593278],\n",
       "        [0.59191618],\n",
       "        [0.52000946],\n",
       "        [0.574164  ],\n",
       "        [0.56645754],\n",
       "        [0.39329587],\n",
       "        [0.3860496 ],\n",
       "        [0.47579106],\n",
       "        [0.50717907],\n",
       "        [0.43708968],\n",
       "        [0.44429478],\n",
       "        [0.58650754],\n",
       "        [0.39023877],\n",
       "        [0.58099596],\n",
       "        [0.58622786],\n",
       "        [0.4815789 ],\n",
       "        [0.58481658],\n",
       "        [0.66291318],\n",
       "        [0.6433931 ],\n",
       "        [0.62276129],\n",
       "        [0.5065302 ],\n",
       "        [0.47004694],\n",
       "        [0.57830438],\n",
       "        [0.49252302],\n",
       "        [0.51503229],\n",
       "        [0.58431613],\n",
       "        [0.6249573 ],\n",
       "        [0.3958261 ],\n",
       "        [0.49322182],\n",
       "        [0.62842685],\n",
       "        [0.5455214 ],\n",
       "        [0.58832175],\n",
       "        [0.56027567],\n",
       "        [0.40944981],\n",
       "        [0.46104278],\n",
       "        [0.43565046],\n",
       "        [0.34016647],\n",
       "        [0.58767317],\n",
       "        [0.4747119 ],\n",
       "        [0.48808485],\n",
       "        [0.45372577],\n",
       "        [0.5144677 ],\n",
       "        [0.42976558],\n",
       "        [0.41394454],\n",
       "        [0.49381374],\n",
       "        [0.60761798],\n",
       "        [0.48248322],\n",
       "        [0.64657447],\n",
       "        [0.5278278 ],\n",
       "        [0.58448203],\n",
       "        [0.36805735],\n",
       "        [0.58879179],\n",
       "        [0.54729195],\n",
       "        [0.36044824],\n",
       "        [0.52664121],\n",
       "        [0.54176321],\n",
       "        [0.5116595 ],\n",
       "        [0.52845883],\n",
       "        [0.61226854],\n",
       "        [0.49974116],\n",
       "        [0.44637362],\n",
       "        [0.46120579],\n",
       "        [0.35631541],\n",
       "        [0.49629276],\n",
       "        [0.58462037],\n",
       "        [0.47778912],\n",
       "        [0.55010046],\n",
       "        [0.51200514],\n",
       "        [0.58611291],\n",
       "        [0.46724788],\n",
       "        [0.42393166],\n",
       "        [0.66757709],\n",
       "        [0.61645478],\n",
       "        [0.49646834],\n",
       "        [0.47778145],\n",
       "        [0.50785433],\n",
       "        [0.42408069],\n",
       "        [0.48592028],\n",
       "        [0.52836971],\n",
       "        [0.54257432],\n",
       "        [0.47061441],\n",
       "        [0.51393499],\n",
       "        [0.49587785],\n",
       "        [0.48428905],\n",
       "        [0.49359537],\n",
       "        [0.38712206],\n",
       "        [0.49007607],\n",
       "        [0.46832551],\n",
       "        [0.62166595],\n",
       "        [0.44336165],\n",
       "        [0.45332582],\n",
       "        [0.45276501],\n",
       "        [0.45861564],\n",
       "        [0.45785585],\n",
       "        [0.46037457],\n",
       "        [0.47661435],\n",
       "        [0.63491638],\n",
       "        [0.44330539],\n",
       "        [0.4217788 ],\n",
       "        [0.58284905],\n",
       "        [0.68737811],\n",
       "        [0.54222266],\n",
       "        [0.57815321],\n",
       "        [0.51827471],\n",
       "        [0.62050637],\n",
       "        [0.53630626],\n",
       "        [0.52184095],\n",
       "        [0.49997   ],\n",
       "        [0.41803917],\n",
       "        [0.63932096],\n",
       "        [0.58844628],\n",
       "        [0.56065519],\n",
       "        [0.55052758],\n",
       "        [0.50206456],\n",
       "        [0.44420918],\n",
       "        [0.65454062],\n",
       "        [0.41694922],\n",
       "        [0.6535918 ],\n",
       "        [0.41305138],\n",
       "        [0.56597782],\n",
       "        [0.48708682],\n",
       "        [0.47189688],\n",
       "        [0.48117885],\n",
       "        [0.4080941 ]]),\n",
       " 0.6217332108567507)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = predict(x_test, theta)\n",
    "y_predict, cost(x_train, y_train, theta, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.38095238095238\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "y_predict_new = np.where(y_predict >= 0.55, 1, 0)\n",
    "for i in range(168):\n",
    "    if y_predict_new[i] == y_test[i]:\n",
    "        cnt += 1\n",
    "print(cnt * 100 / 168) # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamdas = np.arange(0, 1, 0.01)\n",
    "lamdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda 0.0: 0.540639132001548\n",
      "lamda 0.01: 0.53954811101587\n",
      "lamda 0.02: 0.5391541728069034\n",
      "lamda 0.03: 0.5394376243258335\n",
      "lamda 0.04: 0.5403790780669232\n",
      "lamda 0.05: 0.5419420154800537\n",
      "lamda 0.06: 0.54406446327486\n",
      "lamda 0.07: 0.5466589923669318\n",
      "lamda 0.08: 0.5496192277698773\n",
      "lamda 0.09: 0.5528302422474628\n",
      "lamda 0.1: 0.5561798905314461\n",
      "lamda 0.11: 0.5595684976780094\n",
      "lamda 0.12: 0.5629152489711481\n",
      "lamda 0.13: 0.5661607915982944\n",
      "lamda 0.14: 0.5692665447681613\n",
      "lamda 0.15: 0.5722117725246982\n",
      "lamda 0.16: 0.5749895786975888\n",
      "lamda 0.17: 0.5776027752972649\n",
      "lamda 0.18: 0.5800602377464641\n",
      "lamda 0.19: 0.58237403460194\n",
      "lamda 0.2: 0.5845573780636644\n",
      "lamda 0.21: 0.5866232992061605\n",
      "lamda 0.22: 0.5885838910918074\n",
      "lamda 0.23: 0.5904499561103559\n",
      "lamda 0.24: 0.59223091579794\n",
      "lamda 0.25: 0.5939348739549214\n",
      "lamda 0.26: 0.5955687563705436\n",
      "lamda 0.27: 0.5971384777462215\n",
      "lamda 0.28: 0.598649106939876\n",
      "lamda 0.29: 0.6001050158139991\n",
      "lamda 0.3: 0.6015100059636483\n",
      "lamda 0.31: 0.6028674128042273\n",
      "lamda 0.32: 0.6041801891423447\n",
      "lamda 0.33: 0.6054509714001451\n",
      "lamda 0.34: 0.6066821318031003\n",
      "lamda 0.35000000000000003: 0.6078758195283293\n",
      "lamda 0.36: 0.6090339933224356\n",
      "lamda 0.37: 0.6101584475884563\n",
      "lamda 0.38: 0.611250833485029\n",
      "lamda 0.39: 0.6123126762043087\n",
      "lamda 0.4: 0.6133453893002108\n",
      "lamda 0.41000000000000003: 0.6143502867152674\n",
      "lamda 0.42: 0.6153285929890471\n",
      "lamda 0.43: 0.6162814520102492\n",
      "lamda 0.44: 0.6172099345867416\n",
      "lamda 0.45: 0.6181150450439186\n",
      "lamda 0.46: 0.6189977270149999\n",
      "lamda 0.47000000000000003: 0.6198588685523342\n",
      "lamda 0.48: 0.6206993066629124\n",
      "lamda 0.49: 0.6215198313516692\n",
      "lamda 0.5: 0.622321189241021\n",
      "lamda 0.51: 0.6231040868232819\n",
      "lamda 0.52: 0.6238691933932509\n",
      "lamda 0.53: 0.6246171437007557\n",
      "lamda 0.54: 0.6253485403568866\n",
      "lamda 0.55: 0.6260639560226745\n",
      "lamda 0.56: 0.6267639354048872\n",
      "lamda 0.5700000000000001: 0.6274489970802053\n",
      "lamda 0.58: 0.6281196351662026\n",
      "lamda 0.59: 0.6287763208551537\n",
      "lamda 0.6: 0.6294195038246703\n",
      "lamda 0.61: 0.6300496135374352\n",
      "lamda 0.62: 0.630667060440835\n",
      "lamda 0.63: 0.6312722370760238\n",
      "lamda 0.64: 0.6318655191048546\n",
      "lamda 0.65: 0.6324472662621778\n",
      "lamda 0.66: 0.6330178232401773\n",
      "lamda 0.67: 0.6335775205107063\n",
      "lamda 0.68: 0.6341266750909519\n",
      "lamda 0.6900000000000001: 0.6346655912572188\n",
      "lamda 0.7000000000000001: 0.6351945612111258\n",
      "lamda 0.71: 0.6357138657021001\n",
      "lamda 0.72: 0.6362237746096592\n",
      "lamda 0.73: 0.6367245474886548\n",
      "lamda 0.74: 0.6372164340803408\n",
      "lamda 0.75: 0.6376996747918733\n",
      "lamda 0.76: 0.6381745011466103\n",
      "lamda 0.77: 0.6386411362073696\n",
      "lamda 0.78: 0.6390997949746094\n",
      "lamda 0.79: 0.639550684761338\n",
      "lamda 0.8: 0.6399940055463891\n",
      "lamda 0.81: 0.6404299503075784\n",
      "lamda 0.8200000000000001: 0.6408587053361203\n",
      "lamda 0.8300000000000001: 0.6412804505335782\n",
      "lamda 0.84: 0.6416953596925176\n",
      "lamda 0.85: 0.6421036007619381\n",
      "lamda 0.86: 0.6425053360984789\n",
      "lamda 0.87: 0.6429007227043125\n",
      "lamda 0.88: 0.6432899124525747\n",
      "lamda 0.89: 0.6436730523011175\n",
      "lamda 0.9: 0.6440502844953034\n",
      "lamda 0.91: 0.6444217467605232\n",
      "lamda 0.92: 0.6447875724850526\n",
      "lamda 0.93: 0.6451478908938344\n",
      "lamda 0.9400000000000001: 0.6455028272137208\n",
      "lamda 0.9500000000000001: 0.6458525028306794\n",
      "lamda 0.96: 0.6461970354394282\n",
      "lamda 0.97: 0.6465365391859365\n",
      "lamda 0.98: 0.6468711248031962\n",
      "lamda 0.99: 0.6472008997406427\n"
     ]
    }
   ],
   "source": [
    "for lamda in lamdas:\n",
    "    theta = train(x_train, y_train, theta, epochs, l_rate, lamda)\n",
    "    print(\"lamda \" + str(lamda) + \": \" + str(cost(x_train, y_train, theta, lamda)))\n",
    "# Thus lamda 0.02 is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.57142857142857\n"
     ]
    }
   ],
   "source": [
    "theta = train(x_train, y_train, theta, epochs, l_rate, 0.02)\n",
    "y_predict = predict(x_test, theta)\n",
    "cnt = 0\n",
    "y_predict_new = np.where(y_predict >= 0.55, 1, 0)\n",
    "for i in range(168):\n",
    "    if y_predict_new[i] == y_test[i]:\n",
    "        cnt += 1\n",
    "print(cnt * 100 / 168) # Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
